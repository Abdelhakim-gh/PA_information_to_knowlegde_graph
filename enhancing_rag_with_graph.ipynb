{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x3LkpUztHNU",
        "outputId": "b4243a4f-747a-47a1-91af-3d6bf7e89a78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/301.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.7/301.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchain-ollama langchain-experimental neo4j tiktoken yfiles_jupyter_graphs python-dotenv json-repair langchain-openai langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYiTfiOB9W-z"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loO2XHOf-Ffj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk6mtgCw_Jqi"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!ollama pull llama3.1\n",
        "clear_output()\n",
        "\n",
        "!pip install -U lightrag[ollama]\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPIRSGz4tHNV"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.chat_models import ChatOllama  # Mise à jour de l'importation\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from neo4j import GraphDatabase\n",
        "from yfiles_jupyter_graphs import GraphWidget\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_community.document_loaders import PyPDFLoader  # Pour charger les fichiers PDF\n",
        "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
        "from langchain_ollama import OllamaEmbeddings  # À garder si nécessaire"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5-kQwWUWts4",
        "outputId": "856e5e96-3569-4b1f-b400-4268db39b1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd()) # prints your current working directory\n",
        "!ls -l # lists content of your working directory, including files and their permissions."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmDw1apdXYff",
        "outputId": "424fb219-2e7e-4121-c4c9-e3fe335c72a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "total 1912\n",
            "-rw-r--r-- 1 root root 1953257 Nov 20 15:28 progress.json\n",
            "drwxr-xr-x 1 root root    4096 Nov 18 14:23 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGhtLTAStHNW"
      },
      "outputs": [],
      "source": [
        "# # Charger un fichier PDF\n",
        "pdf_loader = PyPDFLoader(file_path=\"data2.pdf\")\n",
        "docs = pdf_loader.load()\n",
        "\n",
        "# Découper les documents en morceaux plus petits pour éviter la surcharge de mémoire\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=24)\n",
        "documents = text_splitter.split_documents(documents=docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Optional\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, id: str, type: str, properties: Dict):\n",
        "        self.id = id\n",
        "        self.type = type\n",
        "        self.properties = properties\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node(id='{self.id}', type='{self.type}', properties={self.properties})\"\n",
        "\n",
        "class Relationship:\n",
        "    def __init__(self, source: Node, target: Node, type: str, properties: Dict):\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "        self.type = type\n",
        "        self.properties = properties\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Relationship(source={self.source}, target={self.target}, type='{self.type}', properties={self.properties})\"\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, metadata: Dict, page_content: str):\n",
        "        self.metadata = metadata\n",
        "        self.page_content = page_content\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Document(metadata={self.metadata}, page_content='{self.page_content[:50]}...')\"\n",
        "\n",
        "class GraphDocument:\n",
        "    def __init__(self, nodes: List[Node], relationships: List[Relationship], source: Document):\n",
        "        self.nodes = nodes\n",
        "        self.relationships = relationships\n",
        "        self.source = source\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"GraphDocument(nodes={self.nodes}, relationships={self.relationships}, source={self.source})\"\n"
      ],
      "metadata": {
        "id": "x9OVUcyab5cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXf7OTGHtHNW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Utilisation du modèle ChatOllama comme avant\n",
        "llm = ChatOllama(model=\"llama3.1\", temperature=0, format=\"json\")  # Utilisation du modèle mis à jour\n",
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Fonction pour convertir un objet Document ou GraphDocument en format sérialisable\n",
        "def serialize_document(doc):\n",
        "    if hasattr(doc, \"to_dict\"):  # Vérifie si l'objet a une méthode to_dict()\n",
        "        return doc.to_dict()\n",
        "    elif hasattr(doc, \"__dict__\"):  # Sérialisation via attributs\n",
        "        return {key: serialize_document(value) if isinstance(value, (list, dict, object)) else value\n",
        "                for key, value in doc.__dict__.items()}\n",
        "    elif isinstance(doc, list):  # Si c'est une liste, itère sur les éléments\n",
        "        return [serialize_document(item) for item in doc]\n",
        "    elif isinstance(doc, dict):  # Si c'est un dictionnaire, sérialise les valeurs\n",
        "        return {key: serialize_document(value) for key, value in doc.items()}\n",
        "    else:\n",
        "        return doc  # Retourne la valeur brute pour les types simples (int, str, etc.)\n",
        "\n",
        "# Initialiser une liste pour stocker les documents transformés\n",
        "all_graph_documents = []\n",
        "\n",
        "# Traiter chaque chunk et sauvegarder après chaque étape\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Traitement du chunk {i + 1} / {len(documents)}...\")\n",
        "\n",
        "    # Convertir le document en format graphique\n",
        "    graph_document = llm_transformer.convert_to_graph_documents([doc])\n",
        "\n",
        "    # Sérialiser les documents transformés\n",
        "    serialized_graph_document = [serialize_document(gd) for gd in graph_document]\n",
        "    all_graph_documents.extend(serialized_graph_document)\n",
        "\n",
        "    # Sauvegarder les progrès dans un fichier JSON\n",
        "    with open(\"progress.json\", \"w\") as f:\n",
        "        json.dump(all_graph_documents, f, indent=4)\n",
        "\n",
        "    print(f\"Chunk {i + 1} traité et sauvegardé. Total transformé : {len(all_graph_documents)}\")\n",
        "\n",
        "print(f\"Traitement terminé. Nombre total de documents transformés : {len(all_graph_documents)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"progress.json\", \"r\") as f:\n",
        "    serialized_documents = json.load(f)\n",
        "\n",
        "# Reconstitution des objets en format GraphDocument\n",
        "\n",
        "# Recréer les objets GraphDocument\n",
        "graph_documents = [\n",
        "    GraphDocument(**doc) if isinstance(doc, dict) else doc\n",
        "    for doc in serialized_documents\n",
        "]"
      ],
      "metadata": {
        "id": "I67D8cSfulqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBtgBOXdyO0C",
        "outputId": "67193776-7837-454e-f66c-00d847fee9a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nodes': [],\n",
              " 'relationships': [],\n",
              " 'source': {'id': None,\n",
              "  'metadata': {'source': 'data2.pdf', 'page': 0},\n",
              "  'page_content': '1774 BULLETIN OFFICIEL Nº 7222 – 30 moharrem 1445 (17-8-2023) \\n \\n \\n \\nArrêté du ministre délégué auprès de la ministre de l’économie et des finances, chargé du budget n° 1692 -23 du 4 hija 1444  (23 juin 2023)',\n",
              "  'type': 'Document'}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "graph_documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def transform_json_to_graph(json_data: List[Dict]) -> List[GraphDocument]:\n",
        "    graph_documents = []\n",
        "\n",
        "    for item in json_data:\n",
        "        nodes = [Node(id=node['id'], type=node['type'], properties=node.get('properties', {}))\n",
        "                 for node in item.get('nodes', [])]\n",
        "\n",
        "        relationships = [\n",
        "            Relationship(\n",
        "                source=Node(id=rel['source']['id'], type=rel['source']['type'], properties=rel['source'].get('properties', {})),\n",
        "                target=Node(id=rel['target']['id'], type=rel['target']['type'], properties=rel['target'].get('properties', {})),\n",
        "                type=rel['type'],\n",
        "                properties=rel.get('properties', {})\n",
        "            ) for rel in item.get('relationships', [])\n",
        "        ]\n",
        "\n",
        "        # Vérifier si nodes ou relationships ne sont pas vides\n",
        "        if not nodes and not relationships:\n",
        "            continue  # Ignorer cet item\n",
        "\n",
        "        source_document = Document(\n",
        "            metadata=item['source']['metadata'],\n",
        "            page_content=item['source']['page_content']\n",
        "        )\n",
        "\n",
        "        graph_documents.append(GraphDocument(nodes=nodes, relationships=relationships, source=source_document))\n",
        "\n",
        "    return graph_documents\n",
        "\n",
        "# Charger le fichier progress.json\n",
        "with open(\"progress.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "# Transformer en GraphDocuments\n",
        "graph_documents = transform_json_to_graph(json_data)\n",
        "\n",
        "# Vérifier la sortie\n",
        "print(f\"Nombre de documents transformés : {len(graph_documents)}\")\n",
        "print(\"Exemple d'un document transformé :\")\n",
        "if graph_documents:\n",
        "  graph_documents[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S09E7xDb71n",
        "outputId": "c9fd00d7-43d6-472d-8add-2cf74f088bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de documents transformés : 669\n",
            "Exemple d'un document transformé :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_documents[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiCBAxQNeJGp",
        "outputId": "c0bfa015-6ce5-4ba2-f7cd-f21e49e6d00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphDocument(nodes=[Node(id='dahir n° 1 -07-129', type='Decree', properties={}), Node(id='loi n° 53 -05', type='Law', properties={})], relationships=[Relationship(source=Node(id='loi n° 53 -05', type='Law', properties={}), target=Node(id='dahir n° 1 -07-129', type='Decree', properties={}), type='RELATED_TO', properties={})], source=Document(metadata={'source': 'data2.pdf', 'page': 0}, page_content='Vu la loi n° 53 -05 relative à l’échange électroni...'))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "SKP_OpAL0K7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://6c05ca87.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"rWJH4DIO0X6a0iHG7844lXj2fFmus0I5NetFqdnGeYo\""
      ],
      "metadata": {
        "id": "6hkEs-gSxdjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull mxbai-embed-large\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "UpEwtSyZvoPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1igUPcdz6k41"
      },
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "from langchain.embeddings import OllamaEmbeddings\n",
        "from langchain.graphs import Neo4jGraph\n",
        "import os\n",
        "\n",
        "# Set up Neo4j connection using environment variables\n",
        "uri = os.environ.get(\"NEO4J_URI\")  # Ensure these environment variables are set\n",
        "user = os.environ.get(\"NEO4J_USERNAME\")\n",
        "password = os.environ.get(\"NEO4J_PASSWORD\")\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
        "\n",
        "\n",
        "\n",
        "# Set up Neo4jGraph and embeddings\n",
        "graph = Neo4jGraph(url=uri, username=user, password=password)\n",
        "\n",
        "#\n",
        "graph.add_graph_documents(\n",
        "    graph_documents,\n",
        "    baseEntityLabel=True,\n",
        "    include_source=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a valid embeddings model (e.g., OllamaEmbeddings)\n",
        "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
        "\n",
        "# Sample usage of graph (without Neo4jVector)\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    embeddings,\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")\n",
        "vector_retriever = vector_index.as_retriever()"
      ],
      "metadata": {
        "id": "aeGzuN5ly1KB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8aa6d93-3714-4f67-d3ba-752b239be009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-c60880a25494>:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
            "  embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
            "ERROR:neo4j.io:[#B1F4]  _: <CONNECTION> error: Failed to read from defunct connection ResolvedIPv4Address(('34.78.243.29', 7687)) (ResolvedIPv4Address(('34.78.243.29', 7687))): OSError('No data')\n",
            "ERROR:neo4j.pool:Unable to retrieve routing information\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 1.1340429605429618s (Unable to retrieve routing information)\n",
            "ERROR:neo4j.pool:Unable to retrieve routing information\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 2.052942471183819s (Unable to retrieve routing information)\n",
            "ERROR:neo4j.pool:Unable to retrieve routing information\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 4.6330616864590715s (Unable to retrieve routing information)\n",
            "ERROR:neo4j.pool:Unable to retrieve routing information\n",
            "WARNING:neo4j.pool:Transaction failed and will be retried in 9.441416115542959s (Unable to retrieve routing information)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMZlhtDmtHNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2287eb7-b32b-4504-8f99-d7ad5433b6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fulltext index created successfully.\n"
          ]
        }
      ],
      "source": [
        "driver = GraphDatabase.driver(\n",
        "        uri = os.environ.get('NEO4J_URI'),\n",
        "        auth = (os.environ.get('NEO4J_USERNAME'), os.environ.get('NEO4J_PASSWORD')))\n",
        "\n",
        "def create_fulltext_index(tx):\n",
        "    query = '''\n",
        "    CREATE FULLTEXT INDEX `fulltext_entity_id`\n",
        "    FOR (n:__Entity__)\n",
        "    ON EACH [n.id];\n",
        "    '''\n",
        "    tx.run(query)\n",
        "\n",
        "# Function to execute the query\n",
        "def create_index():\n",
        "    with driver.session() as session:\n",
        "        session.execute_write(create_fulltext_index)\n",
        "        print(\"Fulltext index created successfully.\")\n",
        "\n",
        "# Call the function to create the index\n",
        "try:\n",
        "    create_index()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Close the driver connection\n",
        "driver.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDhR7GayyO0D"
      },
      "outputs": [],
      "source": [
        "def showGraph() :\n",
        "    driver =GraphDatabase.driver(\n",
        "    uri = os. environ [\"NEO4J_URI\" ],\n",
        "    auth =(os.environ [\"NEO4J_USERNAME\" ],\n",
        "    os. environ [\"NEO4J_PASSWORD\" ]))\n",
        "    session = driver.session()\n",
        "    widget = GraphWidget (graph = session.run(\"MATCH (s)-[r: !MENTIONS] ->(t) RETURN s,r,t\") . graph())\n",
        "    widget.node_label_mapping = 'id'\n",
        "    return widget\n",
        "showGraph ()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field, ValidationError\n",
        "import json\n",
        "\n",
        "\n",
        "class Entities(BaseModel):\n",
        "    \"\"\"Identifying information about entities.\"\"\"\n",
        "    names: list[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the person, organization, or business entities that appear in the text\",\n",
        "    )\n",
        "\n",
        "# Placeholder function to remove special characters for Lucene-compatible queries\n",
        "def remove_lucene_chars(input_text: str) -> str:\n",
        "    # Define characters to remove as needed (e.g., special symbols)\n",
        "    lucene_special_chars = \"!@#$%^&*()[]{}|\\\\;:'\\\"<>,.?/\"\n",
        "    return ''.join(char for char in input_text if char not in lucene_special_chars)\n",
        "\n",
        "# Generates a full-text query for Lucene-based search compatibility\n",
        "def generate_full_text_query(input: str) -> str:\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    if not words:\n",
        "        return \"\"\n",
        "    full_text_query = \" AND \".join([f\"{word}~2\" for word in words])\n",
        "    print(f\"Generated Query: {full_text_query}\")\n",
        "    return full_text_query.strip()\n",
        "\n",
        "# Extracts entities from text using the LLM\n",
        "def extract_entities_from_text(question: str) -> Entities:\n",
        "    # Format the prompt explicitly to ask for a JSON response\n",
        "    formatted_prompt = (\n",
        "        \"You are extracting organization and person entities from the text. \"\n",
        "        \"Extract entities from the following input and return them in JSON format. \"\n",
        "        \"Use the format: {{'names': [...]}}. Input: {question}\"\n",
        "    ).format(question=question)\n",
        "\n",
        "    # Ensure `llm` is initialized and invoke it with the prompt\n",
        "    llm_response = llm.invoke(formatted_prompt)\n",
        "\n",
        "    # Parse the response as JSON and validate it against the Entities model\n",
        "    try:\n",
        "        response_data = json.loads(llm_response.content)  # Parse the JSON response\n",
        "\n",
        "        # Handle nested structure if 'entities' is the outer key\n",
        "        if 'entities' in response_data and 'names' in response_data['entities']:\n",
        "            response_data = response_data['entities']  # Extract only the inner 'names' dictionary\n",
        "\n",
        "        entities = Entities.parse_obj(response_data)  # Validate and convert to Entities model\n",
        "        print(f\"Extracted Entities: {entities}\")\n",
        "        return entities\n",
        "    except (json.JSONDecodeError, ValidationError) as e:\n",
        "        print(\"Error parsing response:\", e)\n",
        "        return Entities(names=[])\n",
        "\n",
        "# Full-text index query and graph retrieval function\n",
        "def graph_retriever(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Collects the neighborhood of entities mentioned in the question\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    # Step 1: Extract entities from the question using LLM\n",
        "    entities = extract_entities_from_text(question)\n",
        "\n",
        "    # Step 2: Retrieve graph data based on each entity\n",
        "    for entity in entities.names:\n",
        "        full_text_query = generate_full_text_query(entity)\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\n",
        "            YIELD node, score\n",
        "            CALL {\n",
        "              WITH node\n",
        "              MATCH (node)-[r:MENTIONS]->(neighbor)\n",
        "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "              UNION ALL\n",
        "              WITH node\n",
        "              MATCH (node)<-[r:MENTIONS]-(neighbor)\n",
        "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
        "            }\n",
        "            RETURN output LIMIT 50\n",
        "            \"\"\",\n",
        "            {\"query\": full_text_query},\n",
        "        )\n",
        "        # Process and format the graph retrieval output\n",
        "        result += \"\\n\".join([el['output'] for el in response])\n",
        "    return result"
      ],
      "metadata": {
        "id": "asKbr1EU4Q93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "_K3maaD0kKTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qulIfqjZkK18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "3lAdYmC3kKTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOllama(model=\"llama3.1\", temperature=0, format=\"json\")  # Utilisation du modèle mis à jour\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YDm670brm8G",
        "outputId": "dde085e8-38d6-4667-f902-c53569048343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-0f415b4df970>:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
            "  llm = ChatOllama(model=\"llama3.1\", temperature=0, format=\"json\")  # Utilisation du modèle mis à jour\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph_retriever(\"Who are candidature and architecte de l'administration?\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRrKKotQ4Sq6",
        "outputId": "cb8560b8-5e03-4237-a677-6fcf47483b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Entities: names=['candidature', 'architecte']\n",
            "Generated Query: candidature~2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 117} for query: \"CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\\n            YIELD node, score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n",
            "WARNING:neo4j.notifications:Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (node, node) { ... }} {position: line: 3, column: 13, offset: 117} for query: \"CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\\n            YIELD node, score\\n            CALL {\\n              WITH node\\n              MATCH (node)-[r:MENTIONS]->(neighbor)\\n              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\\n              UNION ALL\\n              WITH node\\n              MATCH (node)<-[r:MENTIONS]-(neighbor)\\n              RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\\n            }\\n            RETURN output LIMIT 50\\n            \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Query: architecte~2\n",
            "f6e2c9a079b3ce5e9194145086c9f653 - MENTIONS -> candidature\n",
            "08162371cb61ad7d936dcff76efca250 - MENTIONS -> candidaturesc1d755a8674268bd220836cfab7a3713 - MENTIONS -> architects\n",
            "1bd94a7ed285de3f393235b971e3620c - MENTIONS -> architects\n",
            "5c8743a8379e4129f3d09ec6c0fe85bd - MENTIONS -> architecte\n",
            "182361ff8e5cf2cda19de40d86a1b601 - MENTIONS -> architecte\n",
            "1941c437fa0b52a72f51cae0225c6d59 - MENTIONS -> architecte\n",
            "7aad421d5b6e3c1f4bc990cb1cb7f497 - MENTIONS -> architecte\n",
            "c4f5e1e0a9be3460ed1e22642ac95f29 - MENTIONS -> architecte\n",
            "83752e2598141056294e18178f15d870 - MENTIONS -> architecte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCTMp3prtHNX"
      },
      "outputs": [],
      "source": [
        "def full_retriever(question: str):\n",
        "    graph_data = graph_retriever(question)\n",
        "    vector_data = [el.page_content for el in vector_retriever.invoke(question)]\n",
        "\n",
        "    final_data = f\"\"\"Graph data:\n",
        "{graph_data}\n",
        "vector data:\n",
        "{\"#Document \". join(vector_data)}\n",
        "    \"\"\"\n",
        "    return final_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_retriever(\"c'est quoi la relation entre jury et architects?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "2wUjq8HrDp8m",
        "outputId": "9b0e4f83-25d0-42e5-8eeb-45da30a2a2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Entities: names=['jury', 'architects']\n",
            "Generated Query: jury~2\n",
            "Generated Query: architects~2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Graph data:\\ne07ad1c4a7373e1a82cead96a93d56ef - MENTIONS -> Jury\\nae3270dd1167e3b0169ed5e6aeb01cad - MENTIONS -> jury\\n90bb289fc2ed5077292d3800832fc796 - MENTIONS -> jury\\n729083f5f78f134693a58fa65d4aa8b0 - MENTIONS -> jury\\n5164e92d49877a26b7589c4f4f860219 - MENTIONS -> jury\\n08ad51cc7d7163f330af16b488293e15 - MENTIONS -> jury\\n099e2aab90d36fe2336b8b32d8d45b1d - MENTIONS -> jury\\n89886623b034cbc1097f103cee51424f - MENTIONS -> jury\\n262554444647d62a40356d2f3e637355 - MENTIONS -> jury\\nf8f7eadfd72210e3c964fecfc1337d68 - MENTIONS -> jury\\n32f589a16bda2e1f7c89c0991ce6ea80 - MENTIONS -> jury\\n0d6a3e4cbfde2768f20c4e20aaf24143 - MENTIONS -> jury\\n6cb25059752b35a344bc3cfd2727a4e3 - MENTIONS -> jury\\nc1d755a8674268bd220836cfab7a3713 - MENTIONS -> jury\\n4b747a5c15ce0c69d91c002cf8ed203f - MENTIONS -> jury\\n1bd94a7ed285de3f393235b971e3620c - MENTIONS -> juryc1d755a8674268bd220836cfab7a3713 - MENTIONS -> architects\\n1bd94a7ed285de3f393235b971e3620c - MENTIONS -> architects\\n5c8743a8379e4129f3d09ec6c0fe85bd - MENTIONS -> architecte\\n182361ff8e5cf2cda19de40d86a1b601 - MENTIONS -> architecte\\n1941c437fa0b52a72f51cae0225c6d59 - MENTIONS -> architecte\\n7aad421d5b6e3c1f4bc990cb1cb7f497 - MENTIONS -> architecte\\nc4f5e1e0a9be3460ed1e22642ac95f29 - MENTIONS -> architecte\\n83752e2598141056294e18178f15d870 - MENTIONS -> architecte\\nvector data:\\n\\ntext: 6 – A l’issue de ce classement, le président du jury de  concours invite, par lettre recommandée  avec accusé de réception, les \\narchitectes concurrents concernés à:#Document \\ntext: – qu’aucune communication ne soit échangée entre le  maître d’ouvrage et les concurrents enchérisseurs ou  entre les \\nconcurrents enchérisseurs ;#Document \\ntext: 6 – Le président du jury invite, par lettre recommandée  avec accusé de réception, l'architecte concurrent ayant présenté l’offre \\nla plus avantageuse à:#Document \\ntext: l’offre de l’architecte ou des architectes concernés, sous réserve de l’introduction des rectifications nécessaires dans les  conditions \\nprévues au paragraphe 6 de l’article 110 ci–dessous. \\nLe jury arrête, ensuite, la liste:\\n    \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzb2jcittHNY"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Use natural language and be concise.\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "        {\n",
        "            \"context\": full_retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q & A"
      ],
      "metadata": {
        "id": "owK6M-AtPMzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# Set up logging to suppress warnings from Neo4j\n",
        "logging.getLogger(\"neo4j\").setLevel(logging.ERROR)\n"
      ],
      "metadata": {
        "id": "C2oL5krFtyvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "dtU0iMNgtHNY",
        "outputId": "2aa417e8-80a2-4b0c-c2de-ff7b540b5a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Entities: names=['jury', 'architects']\n",
            "Generated Query: jury~2\n",
            "Generated Query: architects~2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{ \"MENTIONS\" : \"architects\", \"TO\" : \"jury\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "chain.invoke(input=\"c'est quoi la relation entre jury et architects?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10PDssPByO0E"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}